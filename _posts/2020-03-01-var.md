---
title: "Value at Risk (I)"
excerpt_separator: "<!--more-->"
categories:
  - Blog
tags:
  - finance
  - risk
---
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

In the second part of FRM exam, the content is divided into four main
 categories of risk: market risk, credit risk, operational risk and liquidity
  risk. In this post, I am going to organize and structure the knowledge with
   regard to Value at Risk, in the Market Risk part.

What is Value at Risk? Value at risk (VaR) is a statistic that measures and 
 quantifies the level of financial risk within a firm, portfolio or position 
  over a specific time frame. VaR modeling determines the potential for loss in 
   the entity being assessed and the probability of occurrence for the defined
    loss.


When VaR is measured in %:

$$ Normal VaR: VaR_{(\alpha)} = - (\mu_{p/l} - z_{\alpha}* \sigma^2) $$

When VaR is measured in $:

$$ Lognormal VaR: VaR_{(\alpha)} = (1 - e^{ \mu_{R} - z_{R}*\sigma_{R
} }) * P_{t-1} $$


### VaR vs ES

Expected shortfall is the average of tail VaRs.


### Non Parametric Approaches

Traiditional Historical Simulation methods have such pros:

- simple & intuitive
- no need for covariance matrices

and cons:
- ghost effect
- slow to reflect major events
- observation data not following risk

1.Age-weighted HS

Age weighted Historical Simulation assigns less weights to aged data, but
  more weights to latest data.
 
 $$ \omega_{(i)} = {\lambda^{i-1}*(1-\lambda) \over 1-\lambda^{n}} $$

where $$\lambda$$ is a decay factor between 0 and 1.
    

2.Volatility-weighted HS

$$\frac{r_{t_{0}}^{*}}{r_{i}} = \frac{\sigma _{T}}{\sigma _{i}} $$


### Parametric Approaches, Extreme Value Theory
Extreme value theory (EVT) is a branch of applied statistics developed to 
address study and predict the probabilities of extreme outcomes. It differs 
from “central tendency” statistics where we seek to dissect probabilities of 
relatively more common events, making use of the central limit theorem. Extreme 
value theory is not governed by the central limit theorem because it deals with 
the tail region of the relevant distribution. Studies on extreme value make use 
of what theory has to offer.

![png]({{site.url}}{{site.baseurl}}/assets/images/var/evt.png)


1.Generalized Extreme Value (GEV)

GEV is used to model the smallest or largest value among a large set of
 independent, identically distributed random values that represent observations.
 
The probability density function for the generalized extreme value distribution 
with location parameter $$\mu$$, scale parameter σ, and shape parameter ξ is:

$$ F(\xi,\mu,\sigma) = exp[ -(1+ \xi(\frac{x-\mu}{\sigma})^{-\frac{1}{\xi
}})] $$, if $$\xi \ne$$ 0 

$$ F(\xi,\mu,\sigma) = exp[ -exp(\frac{x-\mu}{\sigma})] $$, if $$\xi =$$ 0 

For these formulas, the following restriction holds:

$$ (1+ \xi * \frac{x-\mu}{\sigma}) > 0 $$

As noted, the parameter ξ indicates the shape (heaviness) of the tail of the
 limiting distribution.

![png]({{site.url}}{{site.baseurl}}/assets/images/var/evt2.png)


2.Peak-Over-Threshold (POT) Approach

Whereas the generalized extreme value theory provides the natural way to model 
the maxima or minima of a large sample, the peaks-over-threshold approach 
provides the natural way to model exceedances over a high threshold. The POT 
approach (generally) requires fewer parameters than EV approaches based on the 
generalized extreme value theorem.

To see how the POT approach works, let’s define:

- a random variable X as a random iid loss with distribution function F(x), and
- $$u$$ as the threshold value for positive values of X,

In these circumstances, we can define the distribution of excess losses over 
our threshold $$u$$ as:

$$ F_{u}(x) = P \left \{X-u \leqslant x | X > u \right \} = \frac{F(x+u) - F
(u)}{1-F(u)} $$


![png]({{site.url}}{{site.baseurl}}/assets/images/var/gevpot.png)


### Backtesting VaR

The accuracy of VaR model is verified by backtesting techniques. Backtesting is 
the process of comparing losses predicted by a value at risk (VaR) model to 
those actually experienced over the testing period. It is done to ensure that 
VaR models are reasonably accurate.

The overall goal of backtesting is to ensure that actual losses do not exceed
 the expected losses at a given level of confidence. Exceptions are number of
  actual observations over and above the expected level.
  
1.Verifying a model based on expections or failure rates
We verify a model by recording the failure rate which represents the proportion 
of times VaR is exceeded in a given sample.Under the null hypothesis of a 
correctly calibrated model (Null $$H_0$$ : correct model), the number of
 exceptions (x) follows a binomial probability distribution:
 
$$ f(x) = ^{T}C_{x}P^{x}(1-P)^{T-x} $$

where T is the sample size and p is the probability of exception (p=1
-confidence level).

The expected value of (x) is p*T and a variance, 

$$ \sigma ^{2} (x) = p* (1-p)*T $$

2.Model Calibration

To test whether a model is correctly calibrated (Null $$H_0 $$: correct model
), we need to calculate the z-statistic.

$$ z = \frac{(x-pT)}{\sqrt{p(1-p)T}} $$